---
title: "Normalization and Integration"
date: "2024-07-23"
output: 
  html_notebook:
    toc: true
    toc_float: true
    theme: united
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = "center", fig.height = 5, fig.width = 7, warning = F)


library(Seurat)
library(SeuratData)
library(ggplot2)
```

## Load data
```{r}
InstallData("pbmcsca")
obj <- LoadData("pbmcsca")
obj
```

## Basic QC
We now filter cells with more than 1000 features.
```{r}
obj <- subset(obj, nFeature_RNA > 1000)
obj
```
Data contains already the normalized data.

The object contains data from nine different batches (stored in the Method column in the object metadata), representing seven different technologies. The same cell type isolated and sequenced with different technologies.
We can see them:
```{r}
table(obj$Method)
```

## Normalization
The first step after the QC is normalization, to make gene expression comparable between cells. 

As we have multiple batches, we have first to split the object RNA layer into them, and then perform the normalization.

```{r}
obj[["RNA"]] <- split(obj[["RNA"]], f = obj$Method)
obj
```

We can now perform the normalization for each batch with just one line of code:
```{r}
obj <- NormalizeData(obj)
obj
```
This normalization set a scale factor of 10,000 for each cell (so the sum of the counts of a cell is set to 10,000), and adjust the expression value of each gene accordingly. It then calculate the log1p of those values (natural logarithm of the value+1 to account for zeros).
This functionp produces the data layer, from the count layer.

We can confirm that normalization has made the cells expression comparable:
```{r}
apply(obj[["RNA"]]$counts.10x_Chromium_v2[, 1:10], MARGIN = 2, FUN = function(x) {sum(x)})
#take the counts of 10xchromiumv2 of RNA assay, only first 10 cells and calculate the sum of counts.
```


```{r}
apply(obj[["RNA"]]$data.10x_Chromium_v2[, 1:10], MARGIN = 2, FUN = function(x) {sum(exp(x) - 1)})
#the same thing on normalized data: the total count of each cell is the same.
```
From now on, functions will use data, not counts.

## Dimensionality reduction
In order to run the PCA, we have to perform two operations on the data:

* Extrapolate variable features (common variable features across all samples)
* Scale data for those features: when performing PCA, input data should be scaled so that each feature has the same "weight"

```{r}
obj <- FindVariableFeatures(obj)
head(VariableFeatures(obj), 10) #this is working on the layer data.
```

```{r}
obj <- ScaleData(obj) #this works only on the variable features (2k): it creates a common matrix scaling only for the top variable genes.>> I will have a new layer, a scale data (scaled values of the gene expression of the data layer of each sample, for top variable genes)
obj
```

### PCA
```{r}
obj <- RunPCA(obj)
obj
```

To inspect the pca info you can explore the pca in the the list of reductions (in this case there is only one reduction).
e.g.:  obj@reductions$pca@cell.embeddings[1:10,1:5]
e.g.:  obj@reductions$pca@stdev

Now you should plot it, trying to group by all the different covariates that you have that might be sources of unwanted variability (batch, age, sex...), to check that indeed they are not a big source of variability.
```{r}
DimPlot(obj, reduction = "pca", group.by = "Method")
```
The method doesn't seem to be the greatest source of variability, but it might affect gene expression.


To be able to do UMAP, we first do elbow plot of the PCA, to choose the number of dimensions as input of the UMAP. For this we look at the elbow. Generally, between 15 and 25 is okay.There are also methods to calculate the variance associated to each component, and then you choose the number of components that explain x% variability.
```{r}
ElbowPlot(obj, ndims = 50) +
  scale_x_continuous(breaks = seq(0, 50, 5))
```


### UMAP
To better highlight differences in the sample, we can visualize the data with the UMAP. WeÃ¬ll do it with 16 components.
In this way we create a new reduction in the object and we give it a name.If you don't give a name, everytime to do a reduction it will overwrite the reduction that you already have.
```{r}
obj <- RunUMAP(obj, dims = 1:16, reduction = "pca", reduction.name = "umap.unintegrated")
obj
```

We can now visualize it:
```{r}
DimPlot(obj, reduction = "umap.unintegrated", group.by = c("Method"))
```

You should first group by samples and see if they are intermixed. If they are not, there is batch effect. Then you should try to group by also all the other covariates.

We can see that they clusterize by Method. This data should be integrated, as in scRNA seq experiment the clusters should be celltypes.

## Integration
To perform the integration of the data, we will use the `IntegrateLayers` function whithin Seurat. There are different integration methods: CCA, RPCA, Harmony, FastMNN and scVI. We will show CCA and Harmony.

### CCA
```{r}
obj <- IntegrateLayers(
  object = obj, method = CCAIntegration,
  orig.reduction = "pca", new.reduction = "integrated.cca",
  verbose = FALSE
)
obj
#never use UMAP, always use PCA as original reduction.
```
When you integrate, you create a new reduction (so give it a name)!

```{r}
obj <- RunUMAP(obj, dims = 1:16, reduction = "integrated.cca", reduction.name = "umap.cca") #keep the same number of components as before
obj
```
Now we create a new UMAP based on the new reduction, and this new UMAP itself is a new reduction.

```{r, fig.width=12, fig.height=6}
DimPlot(obj, reduction = "umap.cca", group.by = c("Method")) |
  DimPlot(obj, reduction = "umap.unintegrated", group.by = c("Method")) #the old one
```

### Harmony
```{r}
obj <- IntegrateLayers(
  object = obj, method = HarmonyIntegration,
  orig.reduction = "pca", new.reduction = "integrated.harmony",
  verbose = FALSE
)
obj
#very same function as befor, just a different method
```


```{r}
obj <- RunUMAP(obj, dims = 1:16, reduction = "integrated.harmony", reduction.name = "umap.harmony")
obj
```

```{r, fig.height=6, fig.width=12}
DimPlot(obj, reduction = "umap.harmony", group.by = c("Method")) |
  DimPlot(obj, reduction = "umap.cca", group.by = c("Method"))
```

## Extra: SCTNormalization
There is also another normalization method in Seurat, which is an upgraded version of NormalizeData: `SCTransform`. It will replace NormalizeData, FindVariableFeatures and ScaleData functions, after SCTrasform you move directly to PCA. For this, it is important that the object is split by SAMPLE. In theory, for this, one sample is one pool that is loaded in one lane, because it was made for 10X. However, it might be better to split by every single sample. In this dataset, the samples are methods. 
It will introduce a new assay: SCT.

```{r}
obj <- SCTransform(obj, vst.flavor = "v2") #vst.flavor=v2 to use the second version of this algorythm
obj
```
We created a new assay, with already calculated variable features. So you can directly run the PCA on this data.

```{r}
obj <- RunPCA(obj, assay = "SCT", reduction.name = "pca.sct")
obj <- RunUMAP(obj, dims = 1:16, reduction = "pca.sct", reduction.name = "umap.unintegrated.sct")
DimPlot(obj, reduction = "umap.unintegrated.sct", group.by = "Method" )
#here integration is still needed, but it looks better than before.

obj

```
```{r}
obj <- IntegrateLayers(
  object = obj, method = HarmonyIntegration, assay = "SCT",
  orig.reduction = "pca.sct", new.reduction = "integrated.harmony.sct",
  verbose = FALSE
)
obj
obj <- RunUMAP(obj, dims = 1:16, reduction = "integrated.harmony.sct", reduction.name = "umap.harmony.sct")
DimPlot(obj, reduction = "umap.harmony.sct", group.by = "Method")
```


```{r}
devtools::session_info() #at the end of every analysis, to list all the package that you used in the analysis
```

